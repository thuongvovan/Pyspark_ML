{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO - NATURAL LANGUAGE PROCESSING (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= SparkContext(master= 'local', appName= 'Chapter 11 - Demo NLP')\n",
    "ss= SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| id|         sentence|\n",
      "+---+-----------------+\n",
      "|  0|            a b c|\n",
      "|  1|          a, c, a|\n",
      "|  2|a b d, d a. c c b|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df= ss.createDataFrame(\n",
    "    [(0, 'a b c'),\n",
    "    (1, 'a, c, a'),\n",
    "    (2, 'a b d, d a. c c b')],\n",
    "    ['id', 'sentence'])\n",
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chia văn bản thành list các từ viết thường, bởi dấu space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------------------------+-----+\n",
      "|id |sentence         |words                     |count|\n",
      "+---+-----------------+--------------------------+-----+\n",
      "|0  |a b c            |[a, b, c]                 |3    |\n",
      "|1  |a, c, a          |[a,, c,, a]               |3    |\n",
      "|2  |a b d, d a. c c b|[a, b, d,, d, a., c, c, b]|8    |\n",
      "+---+-----------------+--------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "count_token= udf(f= lambda sentence: len(sentence) , returnType= IntegerType())\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "sen_df_by_tokenizer= tokenizer.transform(sen_df).withColumn('count', count_token(col('words')))\n",
    "sen_df_by_tokenizer.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------------------------+-----+\n",
      "|id |sentence         |words                   |count|\n",
      "+---+-----------------+------------------------+-----+\n",
      "|0  |a b c            |[a, b, c]               |3    |\n",
      "|1  |a, c, a          |[a, c, a]               |3    |\n",
      "|2  |a b d, d a. c c b|[a, b, d, d, a, c, c, b]|8    |\n",
      "+---+-----------------+------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "regex_tokenizer= RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", gaps= True,\n",
    "                                 pattern='\\\\W', toLowercase= True)\n",
    "sen_df_by_regextokenizer= regex_tokenizer.transform(sen_df).withColumn('count', count_token(col('words')))\n",
    "sen_df_by_regextokenizer.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "en_stopwords= StopWordsRemover.loadDefaultStopWords('english')\n",
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "stopwords_remover= StopWordsRemover(inputCol= 'words', outputCol= 'words_filtered', stopWords= en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------------------------+-----+------------------+\n",
      "|id |sentence         |words                   |count|words_filtered    |\n",
      "+---+-----------------+------------------------+-----+------------------+\n",
      "|0  |a b c            |[a, b, c]               |3    |[b, c]            |\n",
      "|1  |a, c, a          |[a, c, a]               |3    |[c]               |\n",
      "|2  |a b d, d a. c c b|[a, b, d, d, a, c, c, b]|8    |[b, d, d, c, c, b]|\n",
      "+---+-----------------+------------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df_stopwords_removed= stopwords_remover.transform(sen_df_by_regextokenizer)\n",
    "sen_df_stopwords_removed.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------------------+-----+--------------------+\n",
      "| id|         sentence|               words|count|             n_grams|\n",
      "+---+-----------------+--------------------+-----+--------------------+\n",
      "|  0|            a b c|           [a, b, c]|    3|          [a b, b c]|\n",
      "|  1|          a, c, a|           [a, c, a]|    3|          [a c, c a]|\n",
      "|  2|a b d, d a. c c b|[a, b, d, d, a, c...|    8|[a b, b d, d d, d...|\n",
      "+---+-----------------+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram= NGram(inputCol= 'words', outputCol= 'n_grams', n= 2)\n",
    "ngram.transform(sen_df_by_regextokenizer).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer  \n",
    "Tương đương với TF nhưng được chuẩn hóa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+\n",
      "|words_filtered    |tf_word                  |\n",
      "+------------------+-------------------------+\n",
      "|[b, c]            |(3,[0,1],[1.0,1.0])      |\n",
      "|[c]               |(3,[0],[1.0])            |\n",
      "|[b, d, d, c, c, b]|(3,[0,1,2],[2.0,2.0,2.0])|\n",
      "+------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vectorizer= CountVectorizer(inputCol= 'words_filtered', outputCol= 'tf_word')\n",
    "count_vectorizer_model= count_vectorizer.fit(sen_df_stopwords_removed)\n",
    "count_vectorized= count_vectorizer_model.transform(sen_df_stopwords_removed)\n",
    "count_vectorized.select('words_filtered', 'tf_word').show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF  \n",
    "Vì CountVectorizer đã trả về ma trận số lần xuất hiện của từ được chuẩn hóa theo toàn tập data vì vậy dùng luôn kết quả từ CountVectorizer mà không cần tihs TF nữa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------------------------+\n",
      "|words_filtered    |tf_word                                   |\n",
      "+------------------+------------------------------------------+\n",
      "|[b, c]            |(262144,[28698,30913],[1.0,1.0])          |\n",
      "|[c]               |(262144,[28698],[1.0])                    |\n",
      "|[b, d, d, c, c, b]|(262144,[27526,28698,30913],[2.0,2.0,2.0])|\n",
      "+------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import  HashingTF\n",
    "hashing_tf= HashingTF(inputCol= 'words_filtered', outputCol= 'tf_word', binary= False)\n",
    "tf_df= hashing_tf.transform(sen_df_stopwords_removed)\n",
    "tf_df.select('words_filtered', 'tf_word').show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|idf_word                                               |\n",
      "+-------------------------------------------------------+\n",
      "|(3,[0,1],[0.0,0.28768207245178085])                    |\n",
      "|(3,[0],[0.0])                                          |\n",
      "|(3,[0,1,2],[0.0,0.5753641449035617,1.3862943611198906])|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "idf= IDF(inputCol= 'tf_word', outputCol= 'idf_word')\n",
    "tfidf_model= idf.fit(count_vectorized)\n",
    "tfidf_df= tfidf_model.transform(count_vectorized)\n",
    "tfidf_df.select('idf_word').show(truncate= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
