{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= SparkContext(master= 'local', appName= 'Chapter 11 - Demo NLP')\n",
    "ss= SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df= ss.createDataFrame(\n",
    "    [(0, 'I head about Pysparl'),\n",
    "    (1, 'I know Spark can work well with NLP'),\n",
    "    (2, 'logistic,regression,model,are,supervised')],\n",
    "    ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------------------------------------+------+\n",
      "|sentence                                |word                                        |tokens|\n",
      "+----------------------------------------+--------------------------------------------+------+\n",
      "|I head about Pysparl                    |[i, head, about, pysparl]                   |4     |\n",
      "|I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]|8     |\n",
      "|logistic,regression,model,are,supervised|[logistic,regression,model,are,supervised]  |1     |\n",
      "+----------------------------------------+--------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer= Tokenizer(inputCol= 'sentence', outputCol= 'word')\n",
    "count_token= udf(lambda word: len(word), IntegerType())\n",
    "token_word= tokenizer.transform(sentence_df)\n",
    "token_word.select('sentence', 'word')\\\n",
    ".withColumn('tokens', count_token(col('word'))).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------------+------+\n",
      "|sentence                                |word                                          |tokens|\n",
      "+----------------------------------------+----------------------------------------------+------+\n",
      "|I head about Pysparl                    |[i, head, about, pysparl]                     |4     |\n",
      "|I know Spark can work well with NLP     |[i, know, spark, can, work, well, with, nlp]  |8     |\n",
      "|logistic,regression,model,are,supervised|[logistic, regression, model, are, supervised]|5     |\n",
      "+----------------------------------------+----------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RegexTokenizer\n",
    "regex_tokenizer= RegexTokenizer(inputCol= 'sentence', outputCol= 'word', pattern= '\\\\W')\n",
    "token_word= regex_tokenizer.transform(sentence_df)\n",
    "token_word.select('sentence', 'word')\\\n",
    ".withColumn('tokens', count_token(col('word'))).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|                word|           filltered|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|I head about Pysparl|[i, head, about, ...|     [head, pysparl]|\n",
      "|  1|I know Spark can ...|[i, know, spark, ...|[know, spark, wor...|\n",
      "|  2|logistic,regressi...|[logistic,regress...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StopWordsRemover\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag('en-US'))\n",
    "\n",
    "stopwords_remover= StopWordsRemover(inputCol= 'word', outputCol= 'filltered')\n",
    "token_word_filltered= stopwords_remover.transform(token_word)\n",
    "token_word_filltered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-------------------------------------------------------------------------+\n",
      "|word                                          |filltered                                                                |\n",
      "+----------------------------------------------+-------------------------------------------------------------------------+\n",
      "|[i, head, about, pysparl]                     |[i head, head about, about pysparl]                                      |\n",
      "|[i, know, spark, can, work, well, with, nlp]  |[i know, know spark, spark can, can work, work well, well with, with nlp]|\n",
      "|[logistic, regression, model, are, supervised]|[logistic regression, regression model, model are, are supervised]       |\n",
      "+----------------------------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ngram\n",
    "from pyspark.ml.feature import NGram\n",
    "ngram= NGram(inputCol= 'word', outputCol= 'filltered', n= 2)\n",
    "ngram_word_filltered= ngram.transform(token_word)\n",
    "ngram_word_filltered.select('word', 'filltered').show(truncate= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------------------+\n",
      "|id |sentence           |word                 |\n",
      "+---+-------------------+---------------------+\n",
      "|0  |a, b, c            |[a, b, c]            |\n",
      "|1  |a, b, c, a         |[a, b, c, a]         |\n",
      "|2  |a, b, d, d, a, c, c|[a, b, d, d, a, c, c]|\n",
      "+---+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "sen_df= ss.createDataFrame(\n",
    "    [(0, 'a, b, c'),\n",
    "    (1, 'a, b, c, a'),\n",
    "    (2, 'a, b, d, d, a, c, c')],\n",
    "    ['id', 'sentence'])\n",
    "token_sen_df= regex_tokenizer.transform(sen_df)\n",
    "token_sen_df.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------------------+--------------------------------+\n",
      "|id |sentence           |word                 |rowfeature                      |\n",
      "+---+-------------------+---------------------+--------------------------------+\n",
      "|0  |a, b, c            |[a, b, c]            |(10,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|1  |a, b, c, a         |[a, b, c, a]         |(10,[0,1,2],[2.0,1.0,1.0])      |\n",
      "|2  |a, b, d, d, a, c, c|[a, b, d, d, a, c, c]|(10,[0,1,2,4],[2.0,1.0,2.0,2.0])|\n",
      "+---+-------------------+---------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashing_TF= HashingTF(inputCol= 'word', outputCol= 'rowfeature', numFeatures= 10)\n",
    "featurized_data= hashing_TF.transform(token_sen_df)\n",
    "featurized_data.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------------------------------------+\n",
      "|rowfeature                      |features                                       |\n",
      "+--------------------------------+-----------------------------------------------+\n",
      "|(10,[0,1,2],[1.0,1.0,1.0])      |(10,[0,1,2],[0.0,0.0,0.0])                     |\n",
      "|(10,[0,1,2],[2.0,1.0,1.0])      |(10,[0,1,2],[0.0,0.0,0.0])                     |\n",
      "|(10,[0,1,2,4],[2.0,1.0,2.0,2.0])|(10,[0,1,2,4],[0.0,0.0,0.0,1.3862943611198906])|\n",
      "+--------------------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf= IDF(inputCol= 'rowfeature', outputCol= 'features')\n",
    "idf_model= idf.fit(featurized_data)\n",
    "rescale_data= idf_model.transform(featurized_data)\n",
    "rescale_data.select('rowfeature', 'features').show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------------------+-------------------------------+\n",
      "|id |sentence           |word                 |features                       |\n",
      "+---+-------------------+---------------------+-------------------------------+\n",
      "|0  |a, b, c            |[a, b, c]            |(4,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|1  |a, b, c, a         |[a, b, c, a]         |(4,[0,1,2],[2.0,1.0,1.0])      |\n",
      "|2  |a, b, d, d, a, c, c|[a, b, d, d, a, c, c]|(4,[0,1,2,3],[2.0,2.0,1.0,2.0])|\n",
      "+---+-------------------+---------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vectorizer= CountVectorizer(inputCol= 'word', outputCol= 'features')\n",
    "count_vectorizer_model= count_vectorizer.fit(token_sen_df)\n",
    "count_vectorizer_model.transform(token_sen_df).show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
