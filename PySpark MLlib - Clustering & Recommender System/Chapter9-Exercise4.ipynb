{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 9 - EXERCISE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nhập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= SparkContext(master= 'local', appName= 'Chapter 9 - Exercise 4')\n",
    "ss= SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/Users/vovanthuong/Desktop/9 - Big Data in Machine Learning/Data/Chapter9/LDS9_Data_Day_7/Beauty_5.json'\n",
    "df= ss.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198502"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Số lượng review\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Số lượng item\n",
    "df.select('asin').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22363"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Số lượng reviewerID\n",
    "df.select('reviewerID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.dropna(how= 'any', subset= ['asin', 'reviewerID', 'overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo tập train và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1\n",
    "select= SQLTransformer(statement= 'SELECT asin, reviewerID, overall FROM __THIS__')\n",
    "# 2\n",
    "asin_laber_to_indexer= StringIndexer(inputCol= 'asin', outputCol= 'asin_idx')\n",
    "# 3\n",
    "reviewerID_laber_to_indexer= StringIndexer(inputCol= 'reviewerID', outputCol= 'reviewerID_idx')\n",
    "# 4\n",
    "als= ALS(maxIter= 25, regParam= 0.05,\n",
    "         userCol= 'reviewerID_idx', itemCol= 'asin_idx', ratingCol= 'overall')\n",
    "# Pipe\n",
    "pipe_als= Pipeline(stages= [select, asin_laber_to_indexer, reviewerID_laber_to_indexer, als])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_als_model= pipe_als.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá kết quả của mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator= RegressionEvaluator(labelCol= 'overall', predictionCol= 'prediction',metricName= 'rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trên tập train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18521408810545173"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result= pipe_als_model.transform(train)\n",
    "evaluator.evaluate(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lọc các reviewerID chưa xuất hiện trong tệp train\n",
    "from pyspark.sql.functions import col\n",
    "test= test.filter(col('reviewerID').isin(pipe_als_model.stages[2].labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4488662855124683"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result= pipe_als_model.transform(test)\n",
    "evaluator.evaluate(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomment cho toàn bộ user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomment_for_all= pipe_als_model.stages[3].recommendForAllUsers(numItems= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomment cho một user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.sql.functions import posexplode, col\n",
    "\n",
    "asin_indexer_to_label = IndexToString(inputCol=\"asin_idx\", \n",
    "                                            outputCol=\"asin\", \n",
    "                                            labels= pipe_als_model.stages[1].labels)\n",
    "def recommend_for_user(als_model, reviewerID, numItems):\n",
    "    # Chuyển reviewerID thành reviewerID_idx\n",
    "    reviewerID= ss.createDataFrame([(reviewerID,)], ['reviewerID', ])\n",
    "    reviewerID= pipe_als_model.stages[2].transform(reviewerID)\n",
    "    reviewerID_idx= reviewerID.select('reviewerID_idx').rdd.flatMap(lambda x: x).collect()[0]\n",
    "    # Đề xuất một số lượng item cao nhất cho tất cả user\n",
    "    data= als_model.recommendForAllUsers(numItems= numItems)\n",
    "    # Lọc lại user quan tâm\n",
    "    data= data.where(data.reviewerID_idx == reviewerID_idx)\n",
    "    # Đưa cột reviewerID vào data\n",
    "    data= data.join(reviewerID, on=['reviewerID_idx'], how='inner')\n",
    "    # Chuyển đổi dữ liệu\n",
    "    data= data.select('reviewerID', 'reviewerID_idx',\n",
    "                      posexplode(\"recommendations\").alias('No', 'recommendations'))\n",
    "    data= data.select('reviewerID', 'reviewerID_idx', 'No',\n",
    "                      (col('recommendations')['asin_idx']).alias('asin_idx'),\n",
    "                      (col('recommendations')['rating']).alias('rating'))\n",
    "    data= asin_indexer_to_label.transform(data)\n",
    "    data.show()\n",
    "    asin= data.select('asin')\n",
    "    return asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+--------+---------+----------+\n",
      "|    reviewerID|reviewerID_idx| No|asin_idx|   rating|      asin|\n",
      "+--------------+--------------+---+--------+---------+----------+\n",
      "|A2V5R832QCSOMX|             0|  0|    4830| 5.975937|B000P9FP10|\n",
      "|A2V5R832QCSOMX|             0|  1|    5751| 5.913575|B00447EWNG|\n",
      "|A2V5R832QCSOMX|             0|  2|    8480|5.8183026|B0072AJLNI|\n",
      "|A2V5R832QCSOMX|             0|  3|    9900| 5.817519|B000XTBEY4|\n",
      "|A2V5R832QCSOMX|             0|  4|    7457|5.7841544|B0030HPY74|\n",
      "|A2V5R832QCSOMX|             0|  5|   10803| 5.761909|B001LNOCXQ|\n",
      "|A2V5R832QCSOMX|             0|  6|    9021| 5.755734|B003ZYKUYY|\n",
      "|A2V5R832QCSOMX|             0|  7|   10630| 5.750055|B0068J5K52|\n",
      "|A2V5R832QCSOMX|             0|  8|    6813|5.7496414|B0002B0R14|\n",
      "|A2V5R832QCSOMX|             0|  9|    8700| 5.724177|B000CNIHEG|\n",
      "+--------------+--------------+---+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asin_recomment= recommend_for_user(als_model= pipe_als_model.stages[-1], \n",
    "                                   reviewerID= 'A2V5R832QCSOMX', numItems= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHÁP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/Users/vovanthuong/Desktop/9 - Big Data in Machine Learning/LDS9_DeThi/Du lieu cung cap/ratings_Grocery_and_Gourmet_Food.csv'\n",
    "df_test= ss.read.csv(path, header= False, inferSchema= True)\n",
    "df_test= df_test.toDF('reviewerID', 'asin', 'overall', 'unixReviewTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_test.where(condition= col('reviewerID') == 'A3ABZBEG3KZ0L').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+--------------+\n",
      "|    reviewerID|      asin|overall|unixReviewTime|\n",
      "+--------------+----------+-------+--------------+\n",
      "|A2BSUJYATHI7WW|B002YRBALU|    4.0|    1395619200|\n",
      "|A2BSUJYATHI7WW|B007GPARA0|    4.0|    1395619200|\n",
      "+--------------+----------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.where(condition= col('reviewerID') == 'A2BSUJYATHI7WW').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+--------------+\n",
      "|    reviewerID|      asin|overall|unixReviewTime|\n",
      "+--------------+----------+-------+--------------+\n",
      "|A26LKBXTSIHQV2|1613170416|    5.0|    1398902400|\n",
      "+--------------+----------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.where(condition= col('reviewerID') == 'A26LKBXTSIHQV2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.cache of DataFrame[reviewerID: string, asin: string, overall: double, unixReviewTime: int]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
